# Welcome to my list of interesting projects!

## Agentic frameworks
[Microsoft.TinyTroupe](https://github.com/microsoft/TinyTroupe)
LLM-powered multiagent persona simulation for imagination enhancement and business insights.

[Dynamiq](https://dynamiq-ai.github.io/dynamiq/)
Dynamiq is your all-in-one Gen AI framework, designed to streamline the development of AI-powered applications. Dynamiq specialises in orchestrating retrieval-augmented generation (RAG) and large language model (LLM) agents.

[Agency Swarm](https://vrsen.github.io/agency-swarm/)
we aim to simplify the agent creation process and enable anyone to create collaborative swarm of agents (Agencies), each with distinct roles and capabilities

[Pydantic AI](https://ai.pydantic.dev/)
PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI.

[PhiData](https://www.phidata.com/)
Phidata is a framework for building multi-modal agents and workflows. Build agents with memory, knowledge, tools and reasoning. Build teams of agents that can work together to solve problems. Interact with your agents and workflows using a beautiful Agent UI.

[Smolagents](https://huggingface.co/docs/smolagents/index)
This library is the simplest framework out there to build powerful agents! By the way, wtf are “agents”? We provide our definition in this page, whe’re you’ll also find tips for when to use them or not (spoilers: you’ll often be better off without agents).

[AI Stuite](https://github.com/andrewyng/aisuite)
aisuite makes it easy for developers to use multiple LLM through a standardized interface. Using an interface similar to OpenAI's, aisuite makes it easy to interact with the most popular LLMs and compare the results. It is a thin wrapper around python client libraries, and allows creators to seamlessly swap out and test responses from different LLM providers without changing their code. Today, the library is primarily focussed on chat completions. We will expand it cover more use cases in near future.

[CrewAI-Studio](https://github.com/strnad/CrewAI-Studio)
Welcome to CrewAI Studio! This application provides a user-friendly interface written in Streamlit for interacting with CrewAI, suitable even for those who don't want to write any code. Follow the steps below to install and run the application on Windows or Linux (probably also MacOS) using either Conda or a virtual environment.

## MLOps
[ZenML](https://github.com/zenml-io)
An open-source MLOps + LLMOps framework that seamlessly integrates existing infrastructure and tools

## Ops
[Icecream](https://github.com/gruns/icecream)
Never use print() to debug again

[Pytest](https://docs.pytest.org/en/stable/)
The pytest framework makes it easy to write small, readable tests, and can scale to support complex functional testing for applications and libraries.

[ruff](https://docs.astral.sh/ruff/)
An extremely fast Python linter and code formatter, written in Rust.

[PyRight](https://microsoft.github.io/pyright/#/)
Pyright is a full-featured, standards-compliant static type checker for Python. It is designed for high performance and can be used with large Python source bases.

[UV](https://github.com/astral-sh/uv)
An extremely fast Python package and project manager, written in Rust.

## LLM Memory / caching
[Letta](https://github.com/letta-ai/letta?tab=readme-ov-file)
Letta (formerly MemGPT) is a framework for creating LLM services with memory.

[mem0](https://github.com/mem0ai/mem0)
enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. Mem0 remembers user preferences, adapts to individual needs, and continuously improves over time, making it ideal for customer support chatbots, AI assistants, and autonomous systems.

[memary](https://github.com/kingjulio8238/Memary)
Agents promote human-type reasoning and are a great advancement towards building AGI and understanding ourselves as humans. Memory is a key component of how humans approach tasks and should be weighted the same when building AI agents. memary emulates human memory to advance these agents.

[MemoRAG](https://github.com/qhjqhj00/MemoRAG)
MemoRAG is an innovative RAG framework built on top of a highly efficient, super-long memory model. Unlike standard RAG, which primarily handles queries with explicit information needs, MemoRAG leverages its memory model to achieve a global understanding of the entire database. By recalling query-specific clues from memory, MemoRAG enhances evidence retrieval, resulting in more accurate and contextually rich response generation.​

## State
[Burr](https://burr.dagworks.io/)
Burr makes it easy to develop applications that make decisions (chatbots, agents, simulations, etc...) from simple python building blocks.
Burr works well for any application that uses LLMs, and can integrate with any of your favorite frameworks. Burr includes a UI that can track/monitor/trace your system in real time, along with pluggable persisters (e.g. for memory) to save & load application state.

## LLM inference
[Guidance](https://github.com/guidance-ai/guidance)
Guidance is an efficient programming paradigm for steering language models. With Guidance, you can control how output is structured and get high-quality output for your use case—while reducing latency and cost vs. conventional prompting or fine-tuning. It allows users to constrain generation (e.g. with regex and CFGs) as well as to interleave control (conditionals, loops, tool use) and generation seamlessly.

[llama.cpp](https://github.com/ggerganov/llama.cpp)
The main goal of llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.

## RAG frameworks
[llmware](https://github.com/llmware-ai/llmware)
provides a unified framework for building LLM-based applications (e.g., RAG, Agents), using small, specialized models that can be deployed privately, integrated with enterprise knowledge sources safely and securely, and cost-effectively tuned and adapted for any business process.

## LLM risk assessment
[Guardrails](https://github.com/guardrails-ai/guardrails?tab=readme-ov-file)
Guardrails runs Input/Output Guards in your application that detect, quantify and mitigate the presence of specific types of risks. To look at the full suite of risks, check out [Guardrails Hub](https://hub.guardrailsai.com/).
Guardrails help you generate structured data from LLMs.

## LLM Routers
[Aurelio](https://github.com/aurelio-labs/semantic-router)
Semantic Router is a superfast decision-making layer for your LLMs and agents. Rather than waiting for slow LLM generations to make tool-use decisions, we use the magic of semantic vector space to make those decisions — routing our requests using semantic meaning.

[aisuite](https://github.com/andrewyng/aisuite)
makes it easy for developers to use multiple LLM through a standardized interface. Using an interface similar to OpenAI's, aisuite makes it easy to interact with the most popular LLMs and compare the results. It is a thin wrapper around python client libraries, and allows creators to seamlessly swap out and test responses from different LLM providers without changing their code. 

## rerankers
[rerankers](https://github.com/answerdotai/rerankers)
A lightweight unified API for various reranking models. 

[Voyage Ai](https://www.voyageai.com/)
Supercharging Search and Retrieval for Unstructured Data

[Cohere](https://cohere.com/)
The all-in-one platform for private and secure AI

## Documentation
[mkdocs](https://github.com/squidfunk/mkdocs-material)
Write your documentation in Markdown and create a professional static site for your Open Source or commercial project in minutes – searchable, customizable, more than 60 languages, for all devices.

## MCP
[Model Context Protocol](https://github.com/modelcontextprotocol/servers/tree/main)
A collection of reference implementations and community-contributed servers for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP). This repository showcases the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.

## Distributed LLMs
[Exo](https://github.com/exo-explore/exo)
Run your own AI cluster at home with everyday devices

## Knowledge Graphs
[itext2KG](https://github.com/AuvaLab/itext2kg)
iText2KG is a Python package designed to incrementally construct consistent knowledge graphs with resolved entities and relations by leveraging large language models for entity and relation extraction from text documents. It features zero-shot capability, allowing for knowledge extraction across various domains without specific training. The package includes modules for document distillation, entity extraction, and relation extraction, ensuring resolved and unique entities and relationships. It continuously updates the KG with new documents and integrates them into Neo4j for visual representation.

## Chunking
[Chonkie](https://github.com/chonkie-ai/chonkie)
The no-nonsense RAG chunking library that’s lightweight, lightning-fast, and ready to CHONK your texts.

[Semchunk](https://github.com/umarbutler/semchunk)
A fast and lightweight pure Python library for splitting text into semantically meaningful chunks.

## Data Validation
[Pydantic](https://docs.pydantic.dev/latest/)
Data validation using Python type hints.

## Fine-Tunning
[Unsloth](https://unsloth.ai/)
Makes finetuning large language models like Llama-3, Mistral, Phi-3 and Gemma 2x faster, use 70% less memory, and with no degradation in accuracy!

[MLX](https://opensource.apple.com/projects/mlx/)
MLX is an array framework designed for efficient and flexible machine learning research on Apple silicon.

[Axoloti](https://axoloti.ai/)
MLX is an array framework designed for efficient and flexible machine learning research on Apple silicon.

## Inference
[Groq](https://groq.com/)
We provide fast AI inference in the cloud and in on-prem AI compute centers. We power the speed of iteration, fueling a new wave of innovation, productivity, and discovery. Groq was founded in 2016 to build technology to advance AI because we saw this moment coming. 

[Cerebras](https://cerebras.ai/)
Cerebras Inference Llama 3.3 70B runs at 2,200 tokens/s and Llama 3.1 405B at 969 tokens/s – over 70x faster than GPU clouds. Get instant responses to code-gen, summarization, and agentic tasks.

[Sambanova](https://sambanova.ai/)
We’ve built an enterprise-ready AI platform from the ground up – intentionally designed for the most valuable and complex AI workloads of today and tomorrow. Using our platform to build a technology backbone for the next decade of AI innovation, organizations get pre-trained foundation models that truly transform the way they gain value from AI and deep learning. And, with our flagship offering, SambaNova Suite, we help them realize value 22x faster.

## Code Assistants
[Deepseek](https://chat.deepseek.com/sign_in)

[Groq Labs](https://appgen.groqlabs.com/)

[Projetc X](https://idx.dev/)

[Windsurf](https://codeium.com/windsurf)

[Bolt](https://bolt.new/)

[Lovable](https://lovable.dev/)

[V0](https://v0.dev/)

[Replit](replit.com)

[Bolt.diy](https://stackblitz-labs.github.io/bolt.diy/)
This fork of Bolt.new (oTToDev) allows you to choose the LLM that you use for each prompt! Currently, you can use OpenAI, Anthropic, Ollama, OpenRouter, Gemini, LMStudio, Mistral, xAI, HuggingFace, DeepSeek, or Groq models - and it is easily extended to use any other model supported by the Vercel AI SDK! See the instructions below for running this locally and extending it to include more models.

[Cerebras Code](https://cerebrascoder.com/)

[Codestral](https://mistral.ai/news/codestral/)

[Gemini Coding Partner](https://gemini.google.com/gem/coding-partner)

[Gemini Coder](https://huggingface.co/spaces/osanseviero/gemini-coder)

[Google AI Studio](https://aistudio.google.com/prompts/new_chat)

[Val Town](https://www.val.town/townie/8011cb54-26e0-40b2-b514-64864f3d5dfb)

[Cursor](https://www.cursor.com/)

[Aider](https://aider.chat/)

[CoPilot](https://github.com/features/copilot)

[Cline](https://github.com/cline/cline)

[Continue](https://www.continue.dev/)

[LlamaCode](https://llamacoder.together.ai/)
Together AI code assinstant using Llama and Deekpseek

[HeyBoss](https://www.heyboss.xyz/)
World's 1st AI engineer for non-coders. Type your idea and AI will code your app, sites, and games in minutes, no coding required.

[Trae](https://www.trae.ai/)
Trae is an adaptive AI IDE that transforms how you work, collaborating with you to run faster.

## AI Automation
[n8n](https://n8n.io/)
Secure, AI-native workflow automation. The world's most popular workflow automation platform for technical teams

[Dify](https://dify.ai/)
The Innovation Engine for GenAI Applications

[TriggerDev](https://trigger.dev/)
The open source background jobs platform. Write workflows in normal async code and we’ll handle the rest, from queues to elastic scaling. No timeouts, retries, observability, and zero infrastructure to manage.

[Kestra](https://kestra.io/)
Unified Orchestration Platform to Simplify Business-Critical Workflows and Govern them as Code and from the UI.
